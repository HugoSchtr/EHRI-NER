#!/bin/bash

#SBATCH --job-name=EHRI_NER_multi_large_without_ter    # Job name
#SBATCH --nodes=1                 # node count
#SBATCH --ntasks=1                # total number of tasks across all nodes
#SBATCH --cpus-per-task=16        # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --partition=gpu           # Name of the partition
#SBATCH --gres=gpu:rtx8000:1      # GPU nodes are only available in gpu partition
#SBATCH --hint=multithread        # we get physical cores not logical
#SBATCH --time=24:00:00           # Time limit hrs:min:sec
#SBATCH --output=./log/log_%j.log # Standard output and error log

echo "### Running $SLURM_JOB_NAME ###"

source $HOME/.bashrc
source $HOME/.bash_profile

module purge
module load cuda/10.2.89
module load gnu8/8.3.0

start=`date +%s`

source ./venv/bin/activate

echo ' '
echo ' ,-,'
echo '/.('
echo '\ {'
echo ' `-`'
echo ' '

nvidia-smi

echo ' '
echo ' ,-,'
echo '/.('
echo '\ {'
echo ' `-`'
echo ' '

printenv

echo ' '
echo ' ,-,'
echo '/.('
echo '\ {'
echo ' `-`'
echo ' '

python dataset.py

end=`date +%s`

echo Execution time was `expr $end - $start` seconds.